import os
import json
from openai import OpenAI
from rest_framework.response import Response
from rest_framework.decorators import api_view
from rest_framework import status
from astrapy import DataAPIClient
import dotenv

dotenv.load_dotenv()

# --------------------------------------------------
# CONFIG
# --------------------------------------------------
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ASTRA_TOKEN = os.getenv("ASTRA_TOKEN")
ASTRA_ENDPOINT = os.getenv("ASTRA_ENDPOINT")
COLLECTION_NAME = os.getenv("COLLECTION_NAME")

EMBED_MODEL = "text-embedding-3-small"
CHAT_MODEL = "gpt-4o-mini"

# --------------------------------------------------
# INIT CLIENTS
# --------------------------------------------------
client_ai = OpenAI(api_key=OPENAI_API_KEY)
client_astra = DataAPIClient(ASTRA_TOKEN)
astra_db = client_astra.get_database_by_api_endpoint(ASTRA_ENDPOINT)
astra_collection = astra_db.get_collection(COLLECTION_NAME)


# --------------------------------------------------
# RAG CORE
# --------------------------------------------------
def get_rag_answer(question: str):

    print("\n================ RAG PIPELINE START ================")
    print(" User:", question)

    try:
        # ----------------------------------------------
        # STEP 1 ‚Äî Embedding
        # ----------------------------------------------
        emb = client_ai.embeddings.create(
            model=EMBED_MODEL,
            input=question
        ).data[0].embedding

        print("Embedding created (1536 dims)")

        # ----------------------------------------------
        # STEP 2 ‚Äî VECTOR SEARCH (S·ª¨ D·ª§NG find() V·ªöI $vector SORT ‚Äì C√ÅCH ƒê√öNG C·ª¶A ASTRAPY)
        # ----------------------------------------------
        print("Searching documents by vector similarity...")

        # S·ª≠ d·ª•ng find() v·ªõi sort={"$vector": emb} ‚Äì ƒë√¢y l√† API chu·∫©n cho vector search
        cursor = astra_collection.find(
            sort={"$vector": emb},  # Vector similarity sort
            limit=5,
            include_similarity=True  # L·∫•y ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng (n·∫øu h·ªó tr·ª£)
        )

        results = list(cursor)
        print(f"üìå Found {len(results)} matching docs")

        if len(results) == 0:
            print("‚ö† No RAG match found")
            return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu."

        # ----------------------------------------------
        # STEP 3 ‚Äî Build context
        # ----------------------------------------------
        context_parts = []

        print("\nExtracting context...")
        for doc in results:
            # Similarity ·ªü '$similarity' v·ªõi find() vector
            similarity = doc.get('$similarity', 0)
            print(f"‚úî Context item (similarity: {similarity}): {doc.get('data', {})}")
            d = doc.get("data", {})
            context_parts.append(json.dumps(d, ensure_ascii=False))

        context = "\n---\n".join(context_parts)

        # ----------------------------------------------
        # STEP 4 ‚Äî Build prompt
        # ----------------------------------------------
        system_prompt = (
            "B·∫°n l√† nh√¢n vi√™n t∆∞ v·∫•n c·ªßa h·ªá th·ªëng s·ª≠a xe m√°y. "
            "Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n NG·ªÆ C·∫¢NH ƒë∆∞·ª£c cung c·∫•p. "
            "N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i l·ªãch s·ª± r·∫±ng b·∫°n kh√¥ng bi·∫øt."
        )

        user_prompt = f"""
        --- NG·ªÆ C·∫¢NH ---
        {context}
        --- END ---

        C√¢u h·ªèi kh√°ch h√†ng: {question}
        """

        print("\nSending prompt to OpenAI...")

        # ----------------------------------------------
        # STEP 5 ‚Äî Call ChatGPT
        # ----------------------------------------------
        resp = client_ai.chat.completions.create(
            model=CHAT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3
        )

        answer = resp.choices[0].message.content.strip()

        print("LLM ANSWER:", answer)
        print("================ RAG PIPELINE END ================\n")
        return answer

    except Exception as e:
        print("RAG ERROR:", e)
        import traceback
        traceback.print_exc()  # ƒê·ªÉ debug chi ti·∫øt
        return "ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu. Vui l√≤ng th·ª≠ l·∫°i sau."


# --------------------------------------------------
# API ENDPOINT
# --------------------------------------------------
@api_view(["POST"])
def chat(request):
    question = request.data.get("question", "").strip()

    if not question:
        return Response({"error": "Missing field 'question'."}, status=400)

    answer = get_rag_answer(question)
    return Response({"answer": answer})